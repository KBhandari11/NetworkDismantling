{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THYel5Pj3dSP"
   },
   "source": [
    "# Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "odj1Coq5H080"
   },
   "outputs": [],
   "source": [
    "#@title ##### License { display-mode: \"form\" }\n",
    "# Copyright 2019 DeepMind Technologies Ltd. All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_W_xX2kB6U5g",
    "outputId": "d507c342-0b1b-4c60-c426-7f4fac2af3fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: open_spiel in /home/bhandk/miniconda3/lib/python3.8/site-packages (1.1.0)\n",
      "Requirement already satisfied: absl-py>=0.10.0 in /home/bhandk/miniconda3/lib/python3.8/site-packages (from open_spiel) (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.16 in /home/bhandk/miniconda3/lib/python3.8/site-packages (from open_spiel) (1.22.3)\n",
      "Requirement already satisfied: scipy>=1.5.4 in /home/bhandk/miniconda3/lib/python3.8/site-packages (from open_spiel) (1.8.0)\n",
      "Requirement already satisfied: pip>=20.0.2 in /home/bhandk/miniconda3/lib/python3.8/site-packages (from open_spiel) (21.2.4)\n",
      "Requirement already satisfied: attrs>=19.3.0 in /home/bhandk/miniconda3/lib/python3.8/site-packages (from open_spiel) (21.4.0)\n",
      "Requirement already satisfied: six in /home/bhandk/miniconda3/lib/python3.8/site-packages (from absl-py>=0.10.0->open_spiel) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade open_spiel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MuLMJABdyDtg"
   },
   "source": [
    "# RL Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "GCaO4gSLdwat"
   },
   "outputs": [],
   "source": [
    "# Copyright 2019 DeepMind Technologies Ltd. All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"Reinforcement Learning (RL) Environment for Open Spiel.\n",
    "\n",
    "This module wraps Open Spiel Python interface providing an RL-friendly API. It\n",
    "covers both turn-based and simultaneous move games. Interactions between agents\n",
    "and the underlying game occur mostly through the `reset` and `step` methods,\n",
    "which return a `TimeStep` structure (see its docstrings for more info).\n",
    "\n",
    "The following example illustrates the interaction dynamics. Consider a 2-player\n",
    "Kuhn Poker (turn-based game). Agents have access to the `observations` (a dict)\n",
    "field from `TimeSpec`, containing the following members:\n",
    " * `info_state`: list containing the game information state for each player. The\n",
    "   size of the list always correspond to the number of players. E.g.:\n",
    "   [[0, 1, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0]].\n",
    " * `legal_actions`: list containing legal action ID lists (one for each player).\n",
    "   E.g.: [[0, 1], [0]], which corresponds to actions 0 and 1 being valid for\n",
    "   player 0 (the 1st player) and action 0 being valid for player 1 (2nd player).\n",
    " * `current_player`: zero-based integer representing the player to make a move.\n",
    "\n",
    "At each `step` call, the environment expects a singleton list with the action\n",
    "(as it's a turn-based game), e.g.: [1]. This (zero-based) action must correspond\n",
    "to the player specified at `current_player`. The game (which is at decision\n",
    "node) will process the action and take as many steps necessary to cover chance\n",
    "nodes, halting at a new decision or final node. Finally, a new `TimeStep`is\n",
    "returned to the agent.\n",
    "\n",
    "Simultaneous-move games follow analogous dynamics. The only differences is the\n",
    "environment expects a list of actions, one per player. Note the `current_player`\n",
    "field is \"irrelevant\" here, admitting a constant value defined in spiel.h, which\n",
    "defaults to -2 (module level constant `SIMULTANEOUS_PLAYER_ID`).\n",
    "\n",
    "See open_spiel/python/examples/rl_example.py for example usages.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "\n",
    "import enum\n",
    "from absl import logging\n",
    "import numpy as np\n",
    "\n",
    "import pyspiel\n",
    "\n",
    "SIMULTANEOUS_PLAYER_ID = pyspiel.PlayerId.SIMULTANEOUS\n",
    "\n",
    "class TimeStep(\n",
    "    collections.namedtuple(\n",
    "        \"TimeStep\", [\"observations\", \"rewards\", \"discounts\", \"step_type\"])):\n",
    "  \"\"\"Returned with every call to `step` and `reset`.\n",
    "\n",
    "  A `TimeStep` contains the data emitted by a game at each step of interaction.\n",
    "  A `TimeStep` holds an `observation` (list of dicts, one per player),\n",
    "  associated lists of `rewards`, `discounts` and a `step_type`.\n",
    "\n",
    "  The first `TimeStep` in a sequence will have `StepType.FIRST`. The final\n",
    "  `TimeStep` will have `StepType.LAST`. All other `TimeStep`s in a sequence will\n",
    "  have `StepType.MID.\n",
    "\n",
    "  Attributes:\n",
    "    observations: a list of dicts containing observations per player.\n",
    "    rewards: A list of scalars (one per player), or `None` if `step_type` is\n",
    "      `StepType.FIRST`, i.e. at the start of a sequence.\n",
    "    discounts: A list of discount values in the range `[0, 1]` (one per player),\n",
    "      or `None` if `step_type` is `StepType.FIRST`.\n",
    "    step_type: A `StepType` enum value.\n",
    "  \"\"\"\n",
    "  __slots__ = ()\n",
    "\n",
    "  def first(self):\n",
    "    return self.step_type == StepType.FIRST\n",
    "\n",
    "  def mid(self):\n",
    "    return self.step_type == StepType.MID\n",
    "\n",
    "  def last(self):\n",
    "    return self.step_type == StepType.LAST\n",
    "\n",
    "  def is_simultaneous_move(self):\n",
    "    return self.observations[\"current_player\"] == SIMULTANEOUS_PLAYER_ID\n",
    "\n",
    "  def current_player(self):\n",
    "    return self.observations[\"current_player\"]\n",
    "\n",
    "\n",
    "class StepType(enum.Enum):\n",
    "  \"\"\"Defines the status of a `TimeStep` within a sequence.\"\"\"\n",
    "\n",
    "  FIRST = 0  # Denotes the first `TimeStep` in a sequence.\n",
    "  MID = 1  # Denotes any `TimeStep` in a sequence that is not FIRST or LAST.\n",
    "  LAST = 2  # Denotes the last `TimeStep` in a sequence.\n",
    "\n",
    "  def first(self):\n",
    "    return self is StepType.FIRST\n",
    "\n",
    "  def mid(self):\n",
    "    return self is StepType.MID\n",
    "\n",
    "  def last(self):\n",
    "    return self is StepType.LAST\n",
    "\n",
    "\n",
    "# Global pyspiel members\n",
    "def registered_games():\n",
    "  return pyspiel.registered_games()\n",
    "\n",
    "\n",
    "\n",
    "class ChanceEventSampler(object):\n",
    "  \"\"\"Default sampler for external chance events.\"\"\"\n",
    "\n",
    "  def __init__(self, seed=None):\n",
    "    self.seed(seed)\n",
    "\n",
    "  def seed(self, seed=None):\n",
    "    self._rng = np.random.RandomState(seed)\n",
    "\n",
    "  def __call__(self, state):\n",
    "    \"\"\"Sample a chance event in the given state.\"\"\"\n",
    "    actions, probs = zip(*state.chance_outcomes())\n",
    "    return self._rng.choice(actions, p=probs)\n",
    "\n",
    "\n",
    "class ObservationType(enum.Enum):\n",
    "  \"\"\"Defines what kind of observation to use.\"\"\"\n",
    "  OBSERVATION = 0  # Use observation_tensor\n",
    "  INFORMATION_STATE = 1  # Use information_state_tensor\n",
    "\n",
    "\n",
    "class Environment(object):\n",
    "  \"\"\"Open Spiel reinforcement learning environment class.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               game,\n",
    "               discount=1.0,\n",
    "               chance_event_sampler=None,\n",
    "               observation_type=None,\n",
    "               include_full_state=False,\n",
    "               distribution=None,\n",
    "               mfg_population=None,\n",
    "               enable_legality_check=False,\n",
    "               **kwargs):\n",
    "    \"\"\"Constructor.\n",
    "\n",
    "    Args:\n",
    "      game: [string, pyspiel.Game] Open Spiel game name or game instance.\n",
    "      discount: float, discount used in non-initial steps. Defaults to 1.0.\n",
    "      chance_event_sampler: optional object with `sample_external_events` method\n",
    "        to sample chance events.\n",
    "      observation_type: what kind of observation to use. If not specified, will\n",
    "        default to INFORMATION_STATE unless the game doesn't provide it.\n",
    "      include_full_state: whether or not to include the full serialized\n",
    "        OpenSpiel state in the observations (sometimes useful for debugging).\n",
    "      distribution: the distribution over states if the game is a mean field\n",
    "        game.\n",
    "      mfg_population: The Mean Field Game population to consider.\n",
    "      enable_legality_check: Check the legality of the move before stepping.\n",
    "      **kwargs: dict, additional settings passed to the Open Spiel game.\n",
    "    \"\"\"\n",
    "    self._chance_event_sampler = chance_event_sampler or ChanceEventSampler()\n",
    "    self._include_full_state = include_full_state\n",
    "    self._distribution = distribution\n",
    "    self._mfg_population = mfg_population\n",
    "    self._enable_legality_check = enable_legality_check\n",
    "\n",
    "    if isinstance(game, str):\n",
    "      if kwargs:\n",
    "        game_settings = {key: val for (key, val) in kwargs.items()}\n",
    "        logging.info(\"Using game settings: %s\", game_settings)\n",
    "        self._game = pyspiel.load_game(game, game_settings)\n",
    "      else:\n",
    "        logging.info(\"Using game string: %s\", game)\n",
    "        self._game = pyspiel.load_game(game)\n",
    "    else:  # pyspiel.Game or API-compatible object.\n",
    "      logging.info(\"Using game instance: %s\", game.get_type().short_name)\n",
    "      self._game = game\n",
    "\n",
    "    self._num_players = self._game.num_players()\n",
    "    self._state = None\n",
    "    self._should_reset = True\n",
    "\n",
    "    # Discount returned at non-initial  steps.\n",
    "    self._discounts = [discount] * self._num_players\n",
    "\n",
    "    # Determine what observation type to use.\n",
    "    if observation_type is None:\n",
    "      if self._game.get_type().provides_information_state_tensor:\n",
    "        observation_type = ObservationType.INFORMATION_STATE\n",
    "      else:\n",
    "        observation_type = ObservationType.OBSERVATION\n",
    "\n",
    "    # Check the requested observation type is supported.\n",
    "    if observation_type == ObservationType.OBSERVATION:\n",
    "      if not self._game.get_type().provides_observation_tensor:\n",
    "        raise ValueError(f\"observation_tensor not supported by {game}\")\n",
    "    elif observation_type == ObservationType.INFORMATION_STATE:\n",
    "      if not self._game.get_type().provides_information_state_tensor:\n",
    "        raise ValueError(f\"information_state_tensor not supported by {game}\")\n",
    "    self._use_observation = (observation_type == ObservationType.OBSERVATION)\n",
    "\n",
    "    if self._game.get_type().dynamics == pyspiel.GameType.Dynamics.MEAN_FIELD:\n",
    "      assert distribution is not None\n",
    "      assert mfg_population is not None\n",
    "      assert 0 <= mfg_population < self._num_players\n",
    "\n",
    "  def seed(self, seed=None):\n",
    "    self._chance_event_sampler.seed(seed)\n",
    "\n",
    "  def get_time_step(self):\n",
    "    \"\"\"Returns a `TimeStep` without updating the environment.\n",
    "\n",
    "    Returns:\n",
    "      A `TimeStep` namedtuple containing:\n",
    "        observation: list of dicts containing one observations per player, each\n",
    "          corresponding to `observation_spec()`.\n",
    "        reward: list of rewards at this timestep, or None if step_type is\n",
    "          `StepType.FIRST`.\n",
    "        discount: list of discounts in the range [0, 1], or None if step_type is\n",
    "          `StepType.FIRST`.\n",
    "        step_type: A `StepType` value.\n",
    "    \"\"\"\n",
    "    observations = {\n",
    "        \"num_nodes\":[],\n",
    "        \"info_state\": [],\n",
    "        \"legal_actions\": [],\n",
    "        \"current_player\": [],\n",
    "        \"serialized_state\": []\n",
    "    }\n",
    "    rewards = []\n",
    "    step_type = StepType.LAST if self._state.is_terminal() else StepType.MID\n",
    "    self._should_reset = step_type == StepType.LAST\n",
    "    cur_rewards = self._state.rewards()\n",
    "    for player_id in range(self.num_players):\n",
    "      rewards.append(cur_rewards[player_id])\n",
    "      observations[\"info_state\"].append(self._state.info_state)\n",
    "      '''observations[\"info_state\"].append(\n",
    "          self._state.observation_tensor(player_id) if self._use_observation\n",
    "          else self._state.information_state_tensor(player_id))'''\n",
    "      observations[\"legal_actions\"].append(self._state.legal_actions(player_id))\n",
    "    observations[\"num_nodes\"] = self._state.num_nodes\n",
    "    observations[\"current_player\"] = self._state.current_player()\n",
    "    discounts = self._discounts\n",
    "    if step_type == StepType.LAST:\n",
    "      # When the game is in a terminal state set the discount to 0.\n",
    "      discounts = [0. for _ in discounts]\n",
    "\n",
    "    if self._include_full_state:\n",
    "      observations[\"serialized_state\"] = pyspiel.serialize_game_and_state(\n",
    "          self._game, self._state)\n",
    "\n",
    "    return TimeStep(\n",
    "        observations=observations,\n",
    "        rewards=rewards,\n",
    "        discounts=discounts,\n",
    "        step_type=step_type)\n",
    "\n",
    "  def _check_legality(self, actions):\n",
    "    if self.is_turn_based:\n",
    "      legal_actions = self._state.legal_actions()\n",
    "      if actions[0] not in legal_actions:\n",
    "        raise RuntimeError(f\"step() called on illegal action {actions[0]}\")\n",
    "    else:\n",
    "      for p in range(len(actions)):\n",
    "        legal_actions = self._state.legal_actions(p)\n",
    "        if legal_actions and actions[p] not in legal_actions:\n",
    "          raise RuntimeError(f\"step() by player {p} called on illegal \" +\n",
    "                             f\"action: {actions[p]}\")\n",
    "\n",
    "  def step(self, actions):\n",
    "    \"\"\"Updates the environment according to `actions` and returns a `TimeStep`.\n",
    "\n",
    "    If the environment returned a `TimeStep` with `StepType.LAST` at the\n",
    "    previous step, this call to `step` will start a new sequence and `actions`\n",
    "    will be ignored.\n",
    "\n",
    "    This method will also start a new sequence if called after the environment\n",
    "    has been constructed and `reset` has not been called. Again, in this case\n",
    "    `actions` will be ignored.\n",
    "\n",
    "    Args:\n",
    "      actions: a list containing one action per player, following specifications\n",
    "        defined in `action_spec()`.\n",
    "\n",
    "    Returns:\n",
    "      A `TimeStep` namedtuple containing:\n",
    "        observation: list of dicts containing one observations per player, each\n",
    "          corresponding to `observation_spec()`.\n",
    "        reward: list of rewards at this timestep, or None if step_type is\n",
    "          `StepType.FIRST`.\n",
    "        discount: list of discounts in the range [0, 1], or None if step_type is\n",
    "          `StepType.FIRST`.\n",
    "        step_type: A `StepType` value.\n",
    "    \"\"\"\n",
    "    assert len(actions) == self.num_actions_per_step, (\n",
    "        \"Invalid number of actions! Expected {}\".format(\n",
    "            self.num_actions_per_step))\n",
    "    if self._should_reset:\n",
    "      return self.reset()\n",
    "\n",
    "    if self._enable_legality_check:\n",
    "      self._check_legality(actions)\n",
    "\n",
    "    if self.is_turn_based:\n",
    "      self._state.apply_action(actions[0])\n",
    "    else:\n",
    "      self._state.apply_actions(actions)\n",
    "    self._sample_external_events()\n",
    "\n",
    "    return self.get_time_step()\n",
    "\n",
    "  def reset(self):\n",
    "    \"\"\"Starts a new sequence and returns the first `TimeStep` of this sequence.\n",
    "\n",
    "    Returns:\n",
    "      A `TimeStep` namedtuple containing:\n",
    "        observations: list of dicts containing one observations per player, each\n",
    "          corresponding to `observation_spec()`.\n",
    "        rewards: list of rewards at this timestep, or None if step_type is\n",
    "          `StepType.FIRST`.\n",
    "        discounts: list of discounts in the range [0, 1], or None if step_type\n",
    "          is `StepType.FIRST`.\n",
    "        step_type: A `StepType` value.\n",
    "    \"\"\"\n",
    "    self._should_reset = False\n",
    "    if self._game.get_type(\n",
    "    ).dynamics == pyspiel.GameType.Dynamics.MEAN_FIELD and self._num_players > 1:\n",
    "      self._state = self._game.new_initial_state_for_population(\n",
    "          self._mfg_population)\n",
    "    else:\n",
    "      self._state = self._game.new_initial_state()\n",
    "    self._sample_external_events()\n",
    "\n",
    "    observations = {\n",
    "        \"num_nodes\":[],\n",
    "        \"info_state\": [],\n",
    "        \"legal_actions\": [],\n",
    "        \"current_player\": [],\n",
    "        \"serialized_state\": []\n",
    "    }\n",
    "    for player_id in range(self.num_players):\n",
    "      observations[\"info_state\"].append(self._state.info_state)\n",
    "      '''observations[\"info_state\"].append(\n",
    "          self._state.observation_tensor(player_id) if self._use_observation\n",
    "          else self._state.information_state_tensor(player_id))'''\n",
    "      observations[\"legal_actions\"].append(self._state.legal_actions(player_id))\n",
    "    observations[\"num_nodes\"]= self._state.num_nodes\n",
    "    observations[\"current_player\"] = self._state.current_player()\n",
    "    if self._include_full_state:\n",
    "      observations[\"serialized_state\"] = pyspiel.serialize_game_and_state(\n",
    "          self._game, self._state)\n",
    "\n",
    "    return TimeStep(\n",
    "        observations=observations,\n",
    "        rewards=None,\n",
    "        discounts=None,\n",
    "        step_type=StepType.FIRST)\n",
    "\n",
    "  def _sample_external_events(self):\n",
    "    \"\"\"Sample chance events until we get to a decision node.\"\"\"\n",
    "    while self._state.is_chance_node() or (self._state.current_player()\n",
    "                                           == pyspiel.PlayerId.MEAN_FIELD):\n",
    "      if self._state.is_chance_node():\n",
    "        outcome = self._chance_event_sampler(self._state)\n",
    "        self._state.apply_action(outcome)\n",
    "      if self._state.current_player() == pyspiel.PlayerId.MEAN_FIELD:\n",
    "        dist_to_register = self._state.distribution_support()\n",
    "        dist = [\n",
    "            self._distribution.value_str(str_state, default_value=0.0)\n",
    "            for str_state in dist_to_register\n",
    "        ]\n",
    "        self._state.update_distribution(dist)\n",
    "\n",
    "  def observation_spec(self):\n",
    "    \"\"\"Defines the observation per player provided by the environment.\n",
    "\n",
    "    Each dict member will contain its expected structure and shape. E.g.: for\n",
    "    Kuhn Poker {\"info_state\": (6,), \"legal_actions\": (2,), \"current_player\": (),\n",
    "                \"serialized_state\": ()}\n",
    "\n",
    "    Returns:\n",
    "      A specification dict describing the observation fields and shapes.\n",
    "    \"\"\"\n",
    "    return dict(\n",
    "        info_state=(),\n",
    "        num_nodes=(),\n",
    "        legal_actions=(self._game.num_distinct_actions(),),\n",
    "        current_player=(),\n",
    "        serialized_state=(),\n",
    "    )\n",
    "\n",
    "  def action_spec(self):\n",
    "    \"\"\"Defines per player action specifications.\n",
    "\n",
    "    Specifications include action boundaries and their data type.\n",
    "    E.g.: for Kuhn Poker {\"num_actions\": 2, \"min\": 0, \"max\":1, \"dtype\": int}\n",
    "\n",
    "    Returns:\n",
    "      A specification dict containing per player action properties.\n",
    "    \"\"\"\n",
    "    return dict(\n",
    "        num_actions=self._game.num_distinct_actions(),\n",
    "        min=0,\n",
    "        max=self._game.num_distinct_actions() - 1,\n",
    "        dtype=int,\n",
    "    )\n",
    "\n",
    "  # Environment properties\n",
    "  @property\n",
    "  def use_observation(self):\n",
    "    \"\"\"Returns whether the environment is using the game's observation.\n",
    "\n",
    "    If false, it is using the game's information state.\n",
    "    \"\"\"\n",
    "    return self._use_observation\n",
    "\n",
    "  # Game properties\n",
    "  @property\n",
    "  def name(self):\n",
    "    return self._game.get_type().short_name\n",
    "\n",
    "  @property\n",
    "  def num_players(self):\n",
    "    return self._game.num_players()\n",
    "\n",
    "  @property\n",
    "  def num_actions_per_step(self):\n",
    "    return 1 if self.is_turn_based else self.num_players\n",
    "\n",
    "  # New RL calls for more advanced use cases (e.g. search + RL).\n",
    "  @property\n",
    "  def is_turn_based(self):\n",
    "    return ((self._game.get_type().dynamics\n",
    "             == pyspiel.GameType.Dynamics.SEQUENTIAL) or\n",
    "            (self._game.get_type().dynamics\n",
    "             == pyspiel.GameType.Dynamics.MEAN_FIELD))\n",
    "\n",
    "  @property\n",
    "  def max_game_length(self):\n",
    "    return self._game.max_game_length()\n",
    "\n",
    "  @property\n",
    "  def is_chance_node(self):\n",
    "    return self._state.is_chance_node()\n",
    "\n",
    "  @property\n",
    "  def game(self):\n",
    "    return self._game\n",
    "\n",
    "  def set_state(self, new_state):\n",
    "    \"\"\"Updates the game state.\"\"\"\n",
    "    assert new_state.get_game() == self.game, (\n",
    "        \"State must have been created by the same game.\")\n",
    "    self._state = new_state\n",
    "\n",
    "  @property\n",
    "  def get_state(self):\n",
    "    return self._state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lSYvGMVa1m9q"
   },
   "source": [
    "## Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "GdZD4I7f1mpW"
   },
   "outputs": [],
   "source": [
    "# Copyright 2019 DeepMind Technologies Limited\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"Replay buffer of fixed size with a FIFI replacement policy.\"\"\"\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "  \"\"\"ReplayBuffer of fixed size with a FIFO replacement policy.\n",
    "  Stored transitions can be sampled uniformly.\n",
    "  The underlying datastructure is a ring buffer, allowing 0(1) adding and\n",
    "  sampling.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, replay_buffer_capacity):\n",
    "    self._replay_buffer_capacity = replay_buffer_capacity\n",
    "    self._data = []\n",
    "    self._next_entry_index = 0\n",
    "\n",
    "  def add(self, element):\n",
    "    \"\"\"Adds `element` to the buffer.\n",
    "    If the buffer is full, the oldest element will be replaced.\n",
    "    Args:\n",
    "      element: data to be added to the buffer.\n",
    "    \"\"\"\n",
    "    if len(self._data) < self._replay_buffer_capacity:\n",
    "      self._data.append(element)\n",
    "    else:\n",
    "      self._data[self._next_entry_index] = element\n",
    "      self._next_entry_index += 1\n",
    "      self._next_entry_index %= self._replay_buffer_capacity\n",
    "\n",
    "  def sample(self, num_samples):\n",
    "    \"\"\"Returns `num_samples` uniformly sampled from the buffer.\n",
    "    Args:\n",
    "      num_samples: `int`, number of samples to draw.\n",
    "    Returns:\n",
    "      An iterable over `num_samples` random elements of the buffer.\n",
    "    Raises:\n",
    "      ValueError: If there are less than `num_samples` elements in the buffer\n",
    "    \"\"\"\n",
    "    if len(self._data) < num_samples:\n",
    "      raise ValueError(\"{} elements could not be sampled from size {}\".format(\n",
    "          num_samples, len(self._data)))\n",
    "    return random.sample(self._data, num_samples)\n",
    "\n",
    "  def reset(self):\n",
    "    \"\"\"Resets the contents of the replay buffer.\"\"\"\n",
    "    self._data = []\n",
    "    self._next_entry_index = 0\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self._data)\n",
    "\n",
    "  def __iter__(self):\n",
    "    return iter(self._data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32SGt_aG09dx"
   },
   "source": [
    "## DQN PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "N-QQj8Y6088b"
   },
   "outputs": [],
   "source": [
    "# Copyright 2019 DeepMind Technologies Limited\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"DQN agent implemented in PyTorch.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from torch._C import dtype\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from torch.nn import Flatten, Linear\n",
    "from torch_geometric.utils import convert\n",
    "\n",
    "from open_spiel.python import rl_agent\n",
    "#from open_spiel.python.utils.replay_buffer import ReplayBuffer\n",
    "\n",
    "Transition = collections.namedtuple(\n",
    "    \"Transition\",\n",
    "    \"info_state action reward next_info_state is_final_step legal_actions_mask\")\n",
    "\n",
    "ILLEGAL_ACTION_LOGITS_PENALTY = -1e9\n",
    "\n",
    "\n",
    "class GraphNN(nn.Module):\n",
    "  \"\"\"A simple network built from nn.linear layers.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               feature_size,\n",
    "               hidden_sizes):\n",
    "    \"\"\"Create the MLP.\n",
    "    Args:\n",
    "      input_size: (int) number of inputs\n",
    "      hidden_sizes: (list) sizes (number of units) of each hidden layer\n",
    "      output_size: (int) number of outputs\n",
    "      activate_final: (bool) should final layer should include a ReLU\n",
    "    \"\"\"\n",
    "\n",
    "    super(GraphNN, self).__init__()\n",
    "    self.conv1 = GATv2Conv(feature_size, hidden_sizes) \n",
    "    self.conv2 = GATv2Conv(hidden_sizes,hidden_sizes) \n",
    "    self.linear = nn.Linear(hidden_sizes, 1)\n",
    "  \n",
    "  def forward(self, node_feature, edge_index):\n",
    "    x, edge_index = node_feature, edge_index\n",
    "    x = F.relu(self.conv1(x, edge_index))\n",
    "    x = F.relu(self.conv2(x, edge_index))\n",
    "    x = self.linear(x)\n",
    "    x = torch.softmax(x,dim=0)\n",
    "    return x\n",
    "\n",
    "\n",
    "class DQN(rl_agent.AbstractAgent):\n",
    "  \"\"\"DQN Agent implementation in PyTorch.\n",
    "  See open_spiel/python/examples/breakthrough_dqn.py for an usage example.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               player_id,\n",
    "               state_representation_size,\n",
    "               num_actions,\n",
    "               hidden_layers_sizes=5,\n",
    "               output_layer_size =1,\n",
    "               replay_buffer_capacity=10000,\n",
    "               batch_size=128,\n",
    "               replay_buffer_class=ReplayBuffer,\n",
    "               learning_rate=0.001,\n",
    "               update_target_network_every=1000,\n",
    "               learn_every=10,\n",
    "               discount_factor=1.0,\n",
    "               min_buffer_size_to_learn=1000,\n",
    "               epsilon_start=1.0,\n",
    "               epsilon_end=0.1,\n",
    "               epsilon_decay_duration=int(1e6),\n",
    "               optimizer_str=\"adam\",\n",
    "               loss_str=\"huber\"):\n",
    "    \"\"\"Initialize the DQN agent.\"\"\"\n",
    "\n",
    "    # This call to locals() is used to store every argument used to initialize\n",
    "    # the class instance, so it can be copied with no hyperparameter change.\n",
    "    self._kwargs = locals()\n",
    "\n",
    "    self.player_id = player_id\n",
    "    self._num_actions = num_actions\n",
    "    self.num_feature = state_representation_size\n",
    "    self._layer_sizes = hidden_layers_sizes\n",
    "    self._batch_size = batch_size\n",
    "    self._update_target_network_every = update_target_network_every\n",
    "    self._learn_every = learn_every\n",
    "    self._min_buffer_size_to_learn = min_buffer_size_to_learn\n",
    "    self._discount_factor = discount_factor\n",
    "\n",
    "    self._epsilon_start = epsilon_start\n",
    "    self._epsilon_end = epsilon_end\n",
    "    self._epsilon_decay_duration = epsilon_decay_duration\n",
    "\n",
    "    # TODO(author6) Allow for optional replay buffer config.\n",
    "    if not isinstance(replay_buffer_capacity, int):\n",
    "      raise ValueError(\"Replay buffer capacity not an integer.\")\n",
    "    self._replay_buffer = replay_buffer_class(replay_buffer_capacity)\n",
    "    self._prev_timestep = None\n",
    "    self._prev_action = None\n",
    "\n",
    "    # Step counter to keep track of learning, eps decay and target network.\n",
    "    self._step_counter = 0\n",
    "\n",
    "    # Keep track of the last training loss achieved in an update step.\n",
    "    self._last_loss_value = None\n",
    "\n",
    "    # Create the Q-network instances\n",
    "    self._q_network = GraphNN(state_representation_size, self._layer_sizes) #num_actions\n",
    "\n",
    "    self._target_q_network = GraphNN(state_representation_size, self._layer_sizes)\n",
    "    # Q network outputs approx single feature embedded value = approx q value for each Noder\n",
    "    if loss_str == \"mse\":\n",
    "      self.loss_class = F.mse_loss\n",
    "    elif loss_str == \"huber\":\n",
    "      self.loss_class = F.smooth_l1_loss\n",
    "    else:\n",
    "      raise ValueError(\"Not implemented, choose from 'mse', 'huber'.\")\n",
    "\n",
    "    if optimizer_str == \"adam\":\n",
    "      self._optimizer = torch.optim.Adam(\n",
    "          self._q_network.parameters(), lr=learning_rate)\n",
    "    elif optimizer_str == \"sgd\":\n",
    "      self._optimizer = torch.optim.SGD(\n",
    "          self._q_network.parameters(), lr=learning_rate)\n",
    "    else:\n",
    "      raise ValueError(\"Not implemented, choose from 'adam' and 'sgd'.\")\n",
    "\n",
    "  def step(self, time_step, is_evaluation=False, add_transition_record=True):\n",
    "    \"\"\"Returns the action to be taken and updates the Q-network if needed.\n",
    "    Args:\n",
    "      time_step: an instance of rl_environment.TimeStep.\n",
    "      is_evaluation: bool, whether this is a training or evaluation call.\n",
    "      add_transition_record: Whether to add to the replay buffer on this step.\n",
    "    Returns:\n",
    "      A `rl_agent.StepOutput` containing the action probs and chosen action.\n",
    "    \"\"\"\n",
    "\n",
    "    # Act step: don't act at terminal info states or if its not our turn.\n",
    "    if (not time_step.last()) and (\n",
    "        time_step.is_simultaneous_move() or\n",
    "        self.player_id == time_step.current_player()):\n",
    "      num_nodes = time_step.observations[\"num_nodes\"]\n",
    "      info_state = time_step.observations[\"info_state\"][self.player_id]\n",
    "      legal_actions = time_step.observations[\"legal_actions\"][self.player_id]\n",
    "      epsilon = self._get_epsilon(is_evaluation)\n",
    "      action, probs = self._epsilon_greedy(num_nodes,info_state,legal_actions, epsilon)\n",
    "    else:\n",
    "      action = None\n",
    "      probs = []\n",
    "    #if legal_actions <= 5\n",
    "    # Don't mess up with the state during evaluation.\n",
    "    if not is_evaluation:\n",
    "      self._step_counter += 1\n",
    "\n",
    "      if self._step_counter % self._learn_every == 0:\n",
    "        self._last_loss_value = self.learn()\n",
    "\n",
    "      if self._step_counter % self._update_target_network_every == 0:\n",
    "        # state_dict method returns a dictionary containing a whole state of the\n",
    "        # module.\n",
    "        self._target_q_network.load_state_dict(self._q_network.state_dict())\n",
    "\n",
    "      if self._prev_timestep and add_transition_record:\n",
    "        # We may omit record adding here if it's done elsewhere.\n",
    "        self.add_transition(self._prev_timestep, self._prev_action, time_step)\n",
    "\n",
    "      if time_step.last():  # prepare for the next episode.\n",
    "        self._prev_timestep = None\n",
    "        self._prev_action = None\n",
    "        return\n",
    "      else:\n",
    "        self._prev_timestep = time_step\n",
    "        self._prev_action = action\n",
    "\n",
    "    return rl_agent.StepOutput(action=action, probs=probs)\n",
    "\n",
    "  def add_transition(self, prev_time_step, prev_action, time_step):\n",
    "    \"\"\"Adds the new transition using `time_step` to the replay buffer.\n",
    "    Adds the transition from `self._prev_timestep` to `time_step` by\n",
    "    `self._prev_action`.\n",
    "    Args:\n",
    "      prev_time_step: prev ts, an instance of rl_environment.TimeStep.\n",
    "      prev_action: int, action taken at `prev_time_step`.\n",
    "      time_step: current ts, an instance of rl_environment.TimeStep.\n",
    "    \"\"\"\n",
    "    assert prev_time_step is not None\n",
    "    _num_actions = time_step.observations[\"num_nodes\"]\n",
    "    legal_actions = (time_step.observations[\"legal_actions\"][self.player_id])\n",
    "    legal_actions_mask = np.zeros(_num_actions)\n",
    "    legal_actions_mask[legal_actions] = 1.0\n",
    "    transition = Transition(\n",
    "        info_state=(\n",
    "            prev_time_step.observations[\"info_state\"][self.player_id]),\n",
    "        action=prev_action,\n",
    "        reward=time_step.rewards[self.player_id],\n",
    "        next_info_state=time_step.observations[\"info_state\"][self.player_id],\n",
    "        is_final_step=float(time_step.last()),\n",
    "        legal_actions_mask=legal_actions_mask)\n",
    "    self._replay_buffer.add(transition)\n",
    "\n",
    "  def _epsilon_greedy(self,num_nodes, info_state,legal_actions, epsilon):\n",
    "    \"\"\"Returns a valid epsilon-greedy action and valid action probs.\n",
    "    Action probabilities are given by a softmax over legal q-values.\n",
    "    Args:\n",
    "      info_state: hashable representation of the information state.\n",
    "      legal_actions: list of legal actions at `info_state`.\n",
    "      epsilon: float, probability of taking an exploratory action.\n",
    "    Returns:\n",
    "      A valid epsilon-greedy action and valid action probabilities.\n",
    "    \"\"\"\n",
    "    size = int(num_nodes)\n",
    "    probs = np.zeros(size)\n",
    "    if np.random.rand() < epsilon:\n",
    "      action = np.random.choice(legal_actions)\n",
    "      probs[legal_actions] = 1.0 / len(legal_actions)\n",
    "    else:\n",
    "      q_values = self._q_network(info_state.x,info_state.edge_index).detach()\n",
    "      legal_q_values = q_values[legal_actions]\n",
    "      action = legal_actions[torch.argmax(legal_q_values)]\n",
    "      probs[action] = 1.0\n",
    "    return action, probs\n",
    "\n",
    "  def _get_epsilon(self, is_evaluation, power=1.0):\n",
    "    \"\"\"Returns the evaluation or decayed epsilon value.\"\"\"\n",
    "    if is_evaluation:\n",
    "      return 0.0\n",
    "    decay_steps = min(self._step_counter, self._epsilon_decay_duration)\n",
    "    decayed_epsilon = (\n",
    "        self._epsilon_end + (self._epsilon_start - self._epsilon_end) *\n",
    "        (1 - decay_steps / self._epsilon_decay_duration)**power)\n",
    "    return decayed_epsilon\n",
    "  def max_next_q_value(self,target_q,legal_actions_mask):\n",
    "    illegal_actions = 1 - legal_actions_mask\n",
    "    illegal_logits = illegal_actions * ILLEGAL_ACTION_LOGITS_PENALTY\n",
    "    all_target_q = target_q.numpy()+ illegal_logits\n",
    "    max_target_q = np.amax(all_target_q)\n",
    "    return max_target_q\n",
    "  \n",
    "  def learn(self):\n",
    "    \"\"\"Compute the loss on sampled transitions and perform a Q-network update.\n",
    "    If there are not enough elements in the buffer, no loss is computed and\n",
    "    `None` is returned instead.\n",
    "    Returns:\n",
    "      The average loss obtained on this batch of transitions or `None`.\n",
    "    \"\"\"\n",
    "\n",
    "    if (len(self._replay_buffer) < self._batch_size or\n",
    "        len(self._replay_buffer) < self._min_buffer_size_to_learn):\n",
    "      return None     \n",
    "    transitions = self._replay_buffer.sample(self._batch_size)\n",
    "    actions =[]\n",
    "    rewards = []\n",
    "    are_final_steps = []\n",
    "    q_values = []\n",
    "    target_q_values = []\n",
    "    max_next_q = []\n",
    "    for t in transitions:\n",
    "        info_states = t.info_state\n",
    "        q_values.append(torch.flatten(self._q_network(info_states.x,info_states.edge_index)))\n",
    "        actions.append(t.action)\n",
    "        rewards.append(t.reward)\n",
    "        next_info_states = t.next_info_state \n",
    "        target_q_values.append(torch.flatten(self._target_q_network(next_info_states.x,next_info_states.edge_index)))\n",
    "        are_final_steps.append(t.is_final_step)\n",
    "        max_next_q.append(self.max_next_q_value(target_q_values[-1].detach(),t.legal_actions_mask))\n",
    "    actions = torch.LongTensor(np.array(actions))\n",
    "    rewards = torch.Tensor(np.array(rewards))\n",
    "    are_final_steps = torch.Tensor(np.array(are_final_steps))\n",
    "    max_next_q = torch.Tensor(np.array(max_next_q))\n",
    "    self._q_values = q_values\n",
    "    self._target_q_values = target_q_values\n",
    "    target=[]\n",
    "    prediction=[]\n",
    "    for i in range(self._batch_size):\n",
    "        target.append((rewards[i] + (1 - are_final_steps[i]) * self._discount_factor * max_next_q[i]))\n",
    "        prediction.append(self._q_values[i][actions[i].item()])\n",
    "    target = torch.stack(target)\n",
    "    prediction = torch.stack(prediction)\n",
    "    loss = self.loss_class(prediction, target)\n",
    "    self._optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    self._optimizer.step()\n",
    "    return loss\n",
    "\n",
    "\n",
    "  @property\n",
    "  def q_values(self):\n",
    "    return self._q_values\n",
    "\n",
    "  @property\n",
    "  def replay_buffer(self):\n",
    "    return self._replay_buffer\n",
    "\n",
    "  @property\n",
    "  def loss(self):\n",
    "    return self._last_loss_value\n",
    "\n",
    "  @property\n",
    "  def prev_timestep(self):\n",
    "    return self._prev_timestep\n",
    "\n",
    "  @property\n",
    "  def prev_action(self):\n",
    "    return self._prev_action\n",
    "\n",
    "  @property\n",
    "  def step_counter(self):\n",
    "    return self._step_counter\n",
    "\n",
    "  def get_weights(self):\n",
    "    variables = [m.weight for m in self._q_network.model]\n",
    "    variables.append([m.weight for m in self._target_q_network.model])\n",
    "    return variables\n",
    "\n",
    "  def copy_with_noise(self, sigma=0.0, copy_weights=True):\n",
    "    \"\"\"Copies the object and perturbates it with noise.\n",
    "    Args:\n",
    "      sigma: gaussian dropout variance term : Multiplicative noise following\n",
    "        (1+sigma*epsilon), epsilon standard gaussian variable, multiplies each\n",
    "        model weight. sigma=0 means no perturbation.\n",
    "      copy_weights: Boolean determining whether to copy model weights (True) or\n",
    "        just model hyperparameters.\n",
    "    Returns:\n",
    "      Perturbated copy of the model.\n",
    "    \"\"\"\n",
    "    _ = self._kwargs.pop(\"self\", None)\n",
    "    copied_object = DQN(**self._kwargs)\n",
    "\n",
    "    q_network = getattr(copied_object, \"_q_network\")\n",
    "    target_q_network = getattr(copied_object, \"_target_q_network\")\n",
    "\n",
    "    if copy_weights:\n",
    "      with torch.no_grad():\n",
    "        for q_model in q_network.model:\n",
    "          q_model.weight *= (1 + sigma * torch.randn(q_model.weight.shape))\n",
    "        for tq_model in target_q_network.model:\n",
    "          tq_model.weight *= (1 + sigma * torch.randn(tq_model.weight.shape))\n",
    "    return copied_object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vmICdphwIOXx"
   },
   "source": [
    "# Game Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "LSdfHVnlhIUS"
   },
   "outputs": [],
   "source": [
    "\n",
    "#from open_spiel.python import rl_environment\n",
    "from open_spiel.python.algorithms import random_agent\n",
    "\n",
    "def eval_model(env, trained_agents):\n",
    "    \"\"\"Evaluates `trained_agents` against a new graph.\"\"\"\n",
    "    cur_agents = [agent for agent in trained_agents]\n",
    "    time_step = env.reset()\n",
    "    episode_rewards = []\n",
    "    action_lists = []\n",
    "    i = 0\n",
    "    while not time_step.last():\n",
    "        agents_output = [\n",
    "            agent.step(time_step, is_evaluation=True) for agent in cur_agents\n",
    "        ]\n",
    "        action_list = [agent_output.action for agent_output in agents_output]\n",
    "        action_lists.append(action_list[0])\n",
    "        time_step = env.step([action_list[0],action_list[0]])\n",
    "        i+=1\n",
    "        episode_rewards.append(env.get_state._rewards[0])\n",
    "    lcc = env.get_state.lcc\n",
    "    return episode_rewards, lcc, action_lists\n",
    "\n",
    "def eval_ACTION(env, action_list):\n",
    "    \"\"\"Evaluates the env for given action_list\"\"\"\n",
    "    env.reset()\n",
    "    episode_rewards = []\n",
    "    i = 0\n",
    "    for action in action_list:\n",
    "        time_step = env.step([action,action])\n",
    "        i+=1\n",
    "        episode_rewards.append(env.get_state._rewards[0])\n",
    "    lcc = env.get_state.lcc\n",
    "    return episode_rewards, lcc, action_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENAFq9pTPj9n"
   },
   "source": [
    "### Get the Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 369
    },
    "id": "Z6DjmsoSP-_n",
    "outputId": "8f3d7c4c-54a7-4be6-dec9-5903a276cfc4"
   },
   "outputs": [],
   "source": [
    " # Copyright 2019 DeepMind Technologies Ltd. All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an\n",
    "\n",
    "#  \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "# Lint as python3\n",
    "\"\"\" Graph Attack and Defense implemented in Python.\n",
    "This is a simple demonstration of implementing a game in Python, featuring\n",
    "chance and imperfect information.\n",
    "Python games are significantly slower than C++, but it may still be suitable\n",
    "for prototyping or for small games.\n",
    "It is possible to run C++ algorithms on Python implemented games, This is likely\n",
    "to have good performance if the algorithm simply extracts a game tree and then\n",
    "works with that. It is likely to be poor if the algorithm relies on processing\n",
    "and updating states as it goes, e.g. MCTS.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import random\n",
    "import copy\n",
    "import networkx as nx\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric import utils\n",
    "from open_spiel.python.observation import IIGObserverForPublicInfoGame\n",
    "import pyspiel\n",
    "\n",
    "\n",
    "\n",
    "# Helper functions for game details.\n",
    "\n",
    "def sigmoid(x):\n",
    "  return 1/(1 + math.exp(-x))\n",
    "def identity(x):\n",
    "    return x\n",
    "def sofplus(x):\n",
    "  return math.log(1 + math.exp(x))\n",
    "def molloy_reed(g):\n",
    "  k = np.array(g.degree())[:,1]\n",
    "  k2 = np.power(k,2)\n",
    "  mean = np.mean(k)\n",
    "  if mean ==0:\n",
    "    beta = 0\n",
    "  else:\n",
    "    beta = np.mean(k2)/mean\n",
    "  return beta\n",
    "\n",
    "def features(g):\n",
    "    degree_centrality = list(nx.degree_centrality(g).values())\n",
    "    eigen_centrality = list(nx.eigenvector_centrality(g,tol=1e-03).values())\n",
    "    clustering_coeff = list(nx.clustering(g).values())\n",
    "    core_num = list(nx.core_number(g).values())\n",
    "    pagerank = list(nx.pagerank(g).values())\n",
    "    #x = np.column_stack((degree_centrality,clustering_coeff,pagerank, core_num ))\n",
    "    x = np.column_stack((degree_centrality,eigen_centrality,pagerank,clustering_coeff, core_num ))\n",
    "    scaler = StandardScaler()\n",
    "    x_normed = scaler.fit_transform(x)#Standardize features\n",
    "    active_nodes =  np.where(np.array(list(g.nodes(data=\"active\")))[:,1] == 0)[0]\n",
    "    x_normed[active_nodes,:]=np.zeros(np.shape(x_normed)[1])\n",
    "    x = torch.from_numpy(x_normed.astype(np.float32))\n",
    "    return x\n",
    "\n",
    "def _network_dismantle(board):\n",
    "    largest_cc = len(max(nx.connected_components(board), key=len))\n",
    "    cond = True if len(board.edges()) == 0  else False\n",
    "    return cond, largest_cc\n",
    "\n",
    "def _board_to_string(board):\n",
    "    \"\"\"Returns a string representation of the board.\"\"\"\n",
    "    value = np.array(list(board.nodes(data=\"active\")))\n",
    "    return \" \".join(\"(\"+str(e)+\", \"+str(f)+\")\" for e, f in value)\n",
    "  \n",
    "def reset(graph):\n",
    "    active = 1\n",
    "    nx.set_node_attributes(graph,active, \"active\")\n",
    "    return graph  \n",
    "\n",
    "def visual_evaluation(attacker_differentGraph, attacker_sameTypeGraph):    \n",
    "    file_list = [\"foodweb-baywet\",\"inf-USAir97\",\"moreno_crime_projected\"]\n",
    "    path = \"./FINDER/\"\n",
    "    for name in file_list:\n",
    "        _NUM_PLAYERS = 2\n",
    "        fh = open(\"./real/\"+name+\".txt\", \"rb\")\n",
    "        GRAPH = nx.read_edgelist(fh)\n",
    "        fh.close()\n",
    "        nodes = GRAPH.nodes()\n",
    "        map = {n:int(i) for i, n in enumerate(nodes)}\n",
    "        GRAPH = reset(nx.relabel_nodes(GRAPH, map))\n",
    "        _NUM_CELLS = len(GRAPH)\n",
    "        game_name = \"graph_attack_defend\"\n",
    "\n",
    "        _GAME_TYPE = pyspiel.GameType(\n",
    "            short_name=game_name,\n",
    "            long_name=\"Python Attack Defend\",\n",
    "            dynamics=pyspiel.GameType.Dynamics.SIMULTANEOUS,\n",
    "            chance_mode=pyspiel.GameType.ChanceMode.EXPLICIT_STOCHASTIC,\n",
    "            information=pyspiel.GameType.Information.IMPERFECT_INFORMATION,\n",
    "            utility=pyspiel.GameType.Utility.ZERO_SUM,\n",
    "            reward_model=pyspiel.GameType.RewardModel.REWARDS,\n",
    "            max_num_players=_NUM_PLAYERS,\n",
    "            min_num_players=_NUM_PLAYERS,\n",
    "            provides_information_state_string=True,\n",
    "            provides_information_state_tensor=True,\n",
    "            provides_observation_string=False,\n",
    "            provides_observation_tensor=False,\n",
    "            provides_factored_observation_string=True)\n",
    "\n",
    "        _GAME_INFO = pyspiel.GameInfo(\n",
    "            num_distinct_actions=_NUM_CELLS,\n",
    "            max_chance_outcomes=0,\n",
    "            num_players=2,\n",
    "            min_utility=-1.0,\n",
    "            max_utility=1.0,\n",
    "            utility_sum=0.0,\n",
    "            max_game_length=_NUM_CELLS)\n",
    " \n",
    "        class GraphGame(pyspiel.Game):\n",
    "            \"\"\"A Python version of the Graph game.\"\"\"\n",
    "\n",
    "            def __init__(self, params=None):\n",
    "                super().__init__(_GAME_TYPE, _GAME_INFO, params or dict())\n",
    "\n",
    "            def new_initial_state(self):\n",
    "                \"\"\"Returns a state corresponding to the start of a game.\"\"\"\n",
    "                return GraphState(self)\n",
    "\n",
    "            def make_py_observer(self, iig_obs_type=None, params=None):\n",
    "                \"\"\"Returns an object used for observing game state.\"\"\"\n",
    "                return BoardObserver(params)\n",
    "\n",
    "\n",
    "        class GraphState(pyspiel.State):\n",
    "            \"\"\"A python version of the Tic-Tac-Toe state.\"\"\"\n",
    "            def __init__(self, game):\n",
    "                \"\"\"Constructor; should only be called by Game.new_initial_state.\"\"\"\n",
    "                super().__init__(game)\n",
    "                self._is_terminal = False\n",
    "                self.board = copy.deepcopy(GRAPH)\n",
    "                self.num_nodes = len(self.board)\n",
    "                self.num_feature = 5\n",
    "                self.subGraph = self.board.copy()\n",
    "                self.info_state = utils.from_networkx(self.subGraph)\n",
    "                self.info_state.x = features(self.subGraph)\n",
    "                self._rewards = np.zeros(_NUM_PLAYERS)\n",
    "                self._returns = np.zeros(_NUM_PLAYERS)\n",
    "                self.lcc = [len(max(nx.connected_components(self.board), key=len))]\n",
    "                self.r = []\n",
    "                self.alpha = (1-nx.density(self.board))\n",
    "                self.k = np.array(self.board.degree())[:,1]\n",
    "                self.k2 = np.power(self.k,2)\n",
    "                self.beta = [molloy_reed(self.subGraph)]\n",
    "\n",
    "            # OpenSpiel (PySpiel) API functions are below. This is the standard set that\n",
    "            # should be implemented by every perfect-information sequential-move game.\n",
    "\n",
    "            def current_player(self):\n",
    "                \"\"\"Returns id of the next player to move, or TERMINAL if game is over.\"\"\"\n",
    "                #return pyspiel.PlayerId.TERMINAL if self._is_terminal else pyspiel.PlayerId.SIMULTANEOUS\n",
    "                return pyspiel.PlayerId.TERMINAL if self._is_terminal else pyspiel.PlayerId.SIMULTANEOUS\n",
    "            \n",
    "\n",
    "            def _legal_actions(self, player):\n",
    "                \"\"\"Returns a list of legal actions, sorted in ascending order.\"\"\"\n",
    "                all_nodes = np.array(list(self.board.nodes(data=\"active\")))\n",
    "                action_sequence = np.where(all_nodes[:,1] == 1)[0]\n",
    "                return action_sequence\n",
    "\n",
    "            def _apply_actions(self, actions):\n",
    "                \"\"\"Applies the specified action to the state.\"\"\" \n",
    "                self.r.append(self._rewards[0])\n",
    "                attack_node = actions[0]\n",
    "                defend_node = actions[1]\n",
    "                if (actions[0] == actions[1]):\n",
    "                    self.board.nodes[attack_node][\"active\"] = 0\n",
    "                    self.subGraph.nodes[attack_node][\"active\"] = 0\n",
    "                else: \n",
    "                    self.board.nodes[attack_node][\"active\"] = 0\n",
    "                    self.board.nodes[defend_node][\"active\"] = 2\n",
    "                    self.subGraph.nodes[attack_node][\"active\"] = 0\n",
    "                    self.subGraph.nodes[defend_node][\"active\"] = 2\n",
    "                ebunch = list(self.subGraph.edges(attack_node))\n",
    "                self.subGraph.remove_edges_from(ebunch)\n",
    "                cond, l = _network_dismantle(self.subGraph)\n",
    "                self.info_state = utils.from_networkx(self.subGraph)\n",
    "                self.info_state.x = features(self.subGraph)\n",
    "                beta = molloy_reed(self.subGraph)\n",
    "                reward_1 = identity((self.lcc[-1] - l)/self.lcc[-1])\n",
    "                reward_2 = identity(self.beta[-1] - beta)\n",
    "                self._rewards[0] = ((self.num_nodes-len(self.lcc))/self.num_nodes)*(self.alpha * reward_1 +(1-self.alpha)*reward_2)\n",
    "                self._rewards[1] = -self._rewards[0]\n",
    "                self._returns += self._rewards\n",
    "                self.beta.append(beta)  \n",
    "                self.lcc.append(l)\n",
    "                self._is_terminal = cond\n",
    "\n",
    "\n",
    "            def _action_to_string(self, player, action):\n",
    "                \"\"\"Action -> string.\"\"\"\n",
    "                return \"{}({})\".format(0 if player == 0 else 1, action)\n",
    "\n",
    "            def is_terminal(self):\n",
    "                \"\"\"Returns True if the game is over.\"\"\"\n",
    "                return self._is_terminal\n",
    "\n",
    "            def returns(self):\n",
    "                \"\"\"Total reward for each player over the course of the game so far.\"\"\"\n",
    "                return self._returns\n",
    "            def rewards(self):\n",
    "                \"\"\"Total reward for each player over the course of the game so far.\"\"\"\n",
    "                return self._rewards\n",
    "\n",
    "            def __str__(self):\n",
    "                \"\"\"String for debug purposes. No particular semantics are required.\"\"\"\n",
    "                return _board_to_string(self.board)\n",
    "\n",
    "            def new_initial_state(self):\n",
    "                #self.board = reset(self.board)\n",
    "                self.board = copy.deepcopy(GRAPH)\n",
    "                self.subGraph = self.board.copy()\n",
    "                self.info_state = utils.from_networkx(self.subGraph)\n",
    "                self.info_state.x = features(self.subGraph)\n",
    "                self.lcc = [len(max(nx.connected_components(self.board), key=len))]\n",
    "                self.r = []\n",
    "                self.beta = [molloy_reed(self.subGraph)]\n",
    "\n",
    "        class BoardObserver:\n",
    "            \"\"\"Observer, conforming to the PyObserver interface (see observation.py).\"\"\"\n",
    "\n",
    "            def __init__(self,params):\n",
    "                \"\"\"Initializes an empty observation tensor.\"\"\"\n",
    "                if params:\n",
    "                    raise ValueError(f\"Observation parameters not supported; passed {params}\")\n",
    "                # The observation should contain a 1-D tensor in `self.t ensor` and a\n",
    "                # dictionary of views onto the tensor, which may be of any shape.\n",
    "                # Here the observation is indexed `(cell state, row, column)\n",
    "                self.tensor = np.array([])\n",
    "                self.dict = {\"observation\":self.tensor}\n",
    "\n",
    "\n",
    "            def set_from(self, state, player):\n",
    "                \"\"\"Updates `tensor` and `dict` to reflect `state` from PoV of `player`.\"\"\"\n",
    "                # We update the observation via the shaped tensor since indexing is more\n",
    "                # convenient than with the 1-D tensor. Both are views onto the same memory.\n",
    "                obs = self.dict[\"observation\"]\n",
    "                obs = np.zeros((state.num_nodes))\n",
    "                all_nodes = np.array(list(state.board.nodes(data=\"active\")))\n",
    "                for i,x in all_nodes:\n",
    "                    obs[i] = x\n",
    "                self.tensor =obs.flatten()\n",
    "                return self.tensor\n",
    "\n",
    "            def string_from(self, state, player):\n",
    "                \"\"\"Observation of `state` from the PoV of `player`, as a string.\"\"\"\n",
    "                return _board_to_string(state.board)\n",
    "\n",
    "            # Register the game with the OpenSpiel library\n",
    "        pyspiel.register_game(_GAME_TYPE, GraphGame)\n",
    "        game = game_name \n",
    "        env = Environment(game)\n",
    "        num_actions = env.action_spec()[\"num_actions\"]\n",
    "\n",
    "        # Attacker trained alone\n",
    "        if attacker_differentGraph != None:\n",
    "            agents = [attacker_differentGraph,random_agent.RandomAgent(player_id=1, num_actions=num_actions)]\n",
    "            rewards_differentGraph, lcc_differentGraph, actions = eval_model(env, agents)\n",
    "            print(\"Trained Attacker\",actions)\n",
    "        \n",
    "        # Attacker trained alone same graph type\n",
    "        if attacker_sameTypeGraph != None:\n",
    "            agents = [attacker_sameTypeGraph,random_agent.RandomAgent(player_id=1, num_actions=num_actions)]\n",
    "            rewards_sameGraphType, lcc_sameGraphType, actions = eval_model(env, agents)\n",
    "            print(\"Trained Attacker [BA model]\",actions)\n",
    "        \n",
    "        # random agents for evaluation\n",
    "        agents = [random_agent.RandomAgent(player_id=0, num_actions=num_actions),random_agent.RandomAgent(player_id=1, num_actions=num_actions)]\n",
    "        rewards_random, lcc_random, _ = eval_model(env, agents)\n",
    "        \n",
    "        #FINDER MODEl\n",
    "        fname = path+name+\".txt\"\n",
    "        action_list = np.loadtxt(fname, dtype=int)\n",
    "        action_list= [map.get(str(a)) for a in action_list]\n",
    "        rewards_Finder, lcc_Finder, actions = eval_ACTION(env, action_list)\n",
    "        print(\"FINDER [BA model]\",actions)\n",
    "        \n",
    "        #Plot LCC Decrease\n",
    "        if attacker_differentGraph != None: plt.plot(lcc_differentGraph, 'green',label='Trained Attacker')\n",
    "        if attacker_sameTypeGraph != None: plt.plot(lcc_sameGraphType, 'red',label='Trained Attacker [BA model]')\n",
    "        plt.plot(lcc_Finder, 'blue',label='FINDER [BA model]')\n",
    "        plt.plot(lcc_random, 'orange', label='Random Agent')\n",
    "        plt.legend()\n",
    "        plt.title(\"LCC vs No. of Nodes for Agents  -\"+ name +\" Dataset\")\n",
    "        plt.savefig(\"./figure/LCC_vs_No_Nodes_\"+name)\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "        ## Plot Reward\n",
    "        if attacker_differentGraph != None: plt.plot(np.cumsum(rewards_differentGraph), 'green',label='Trained Attacker')\n",
    "        if attacker_sameTypeGraph != None: plt.plot(np.cumsum(rewards_sameGraphType), 'red',label='Trained Attacker [BA model]')\n",
    "        plt.plot(np.cumsum(rewards_Finder), 'blue',label='FINDER [BA model]')\n",
    "        plt.plot(np.cumsum(rewards_random), 'orange', label='Random Agent')\n",
    "        plt.legend()\n",
    "        plt.title(\"Rewards for Agents -\"+ name +\" Dataset\")\n",
    "        plt.savefig(\"./figure/Rewards_\"+name)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attacker_differentGraph= torch.load('./model/model_10000') \n",
    "attacker_sameTypeGraph= torch.load('./model/model_same_10000')\n",
    "visual_evaluation(attacker_differentGraph, attacker_sameTypeGraph)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "32SGt_aG09dx"
   ],
   "machine_shape": "hm",
   "name": "All_Evaluation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "MiniConda List",
   "language": "python",
   "name": "ex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
